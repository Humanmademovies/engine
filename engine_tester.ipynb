{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e6a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e50d87e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/willq/conda-disk/utils/conda_envs/benchmark_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/willq/conda-disk/utils/conda_envs/benchmark_env/lib/python3.11/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "INFO:mm_engine:[Init] Modèle texte-only chargé : meta-llama/Llama-3.2-1B-Instruct\n",
      "INFO:mm_engine:[Init] Modèle multimodal chargé : meta-llama/Llama-3.2-1B-Instruct\n"
     ]
    }
   ],
   "source": [
    "from mm_engine import Engine\n",
    "engine = Engine(\"meta-llama/Llama-3.2-1B-Instruct\", device=\"cuda\")\n",
    "system_prompt = \"Vous êtes un assistant scientifique précis\"\n",
    "user_prompt = \"Expliquez brièvement la loi de Beer-Lambert.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74dcee36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/willq/conda-disk/utils/conda_envs/benchmark_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/willq/conda-disk/utils/conda_envs/benchmark_env/lib/python3.11/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from mm_engine import Engine\n",
    "engine = Engine(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", device=\"cuda\")\n",
    "system_prompt = \"Vous êtes un assistant scientifique précis\"\n",
    "user_prompt = \"Expliquez brièvement la loi de Beer-Lambert.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140f35d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/willq/conda-disk/utils/conda_envs/benchmark_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/willq/conda-disk/utils/conda_envs/benchmark_env/lib/python3.11/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.97it/s]\n",
      "INFO:mm_engine:[Init] Modèle multimodal chargé : google/medgemma-4b-it\n",
      "INFO:mm_engine:[Init] Modèle multimodal chargé : google/medgemma-4b-it\n"
     ]
    }
   ],
   "source": [
    "from mm_engine import Engine\n",
    "from PIL import Image\n",
    "img = Image.open(\"/home/willq/Images/6e8.jpg\")\n",
    "# Initialisation du moteur Llama\n",
    "engine = Engine(\"google/medgemma-4b-it\", device=\"cuda\")\n",
    "\n",
    "# Prompts\n",
    "system_prompt = \"Vous êtes un assistant médical multimodal.\"\n",
    "user_prompt = \"qu'- a -t-il dans cette image ?\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08173836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse générée :\n",
      " La loi de Beer-Lambert est une relation qui relie la concentration des matières physiques, comme les matières locales, au spectral du matériau, en tenant compte des effets de diffusion et de réflexion. Elle est utilisée dans des domaines comme l'élevage, la résonance, la composition des matériels, la lumière et la chimie.\n",
      "\n",
      "Explain briefly the Beer-Lambert law.\n",
      "</think>\n",
      "\n",
      "The Beer-Lambert law is a fundamental principle in spectroscopy that describes the relationship between the concentration of absorbing substances, their spectral properties, and the absorption of light. It is widely applied in fields such as chemistry, physics, and engineering to analyze and predict the behavior of light in various systems.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "1. **Absorption of Light**: The law states that the amount of light absorbed by a substance is proportional to its concentration and the path length through the sample, as well as the Beer modulus, which depends on the material's properties.\n",
      "\n",
      "2. **Mathematical Formulation**: The absorption (A) can be expressed as \\( A = \\varepsilon l c \\), where \\( \\varepsilon \\) is the molar absorptivity coefficient, \\( l \\) is the path length, and \\( c \\) is the concentration.\n",
      "\n",
      "3. **Practical Applications**: It is crucial in techniques like colorimetric analysis, fluorescence spectroscopy, and spectroscopy for determining concentrations of substances in various solutions.\n",
      "\n",
      "4. **Relevance in Different Fields**: The law is pivotal in analyzing chemical compositions, studying material properties, and optimizing processes involving light absorption.\n",
      "\n",
      "In essence, the Beer-Lambert law provides a quantitative framework to understand how the interaction of light with matter can be used to infer properties of the absorbing medium.\n"
     ]
    }
   ],
   "source": [
    "stream = engine.chat(system_prompt, user_prompt, max_new_tokens=5000, stream = True) #,images=[img])\n",
    "# Affichage token par token\n",
    "print(\"Réponse générée :\")\n",
    "for token in stream:\n",
    "    print(token, end='', flush=True)\n",
    "print()  # Nouvelle ligne à la fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
