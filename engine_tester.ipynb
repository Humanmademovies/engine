{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e6a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d87e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import Engine\n",
    "engine = Engine(\"meta-llama/Llama-3.2-1B-Instruct\", device=\"cuda\")\n",
    "system_prompt = \"Vous êtes un assistant scientifique précis\"\n",
    "user_prompt = \"Expliquez brièvement la loi de Beer-Lambert.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dcee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import Engine\n",
    "engine = Engine(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", device=\"cuda\")\n",
    "system_prompt = \"Vous êtes un assistant scientifique précis\"\n",
    "user_prompt = \"Expliquez brièvement la loi de Beer-Lambert.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140f35d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/willq/conda-disk/utils/conda_envs/benchmark_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/willq/conda-disk/utils/conda_envs/benchmark_env/lib/python3.11/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "INFO:mm_engine:[Init] Support vision détecté pour google/medgemma-4b-it\n",
      "INFO:mm_engine:[Init] Token image utilisé : <start_of_image>\n",
      "INFO:mm_engine:[Init] Chargement tokenizer : google/medgemma-4b-it\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.24it/s]\n",
      "INFO:mm_engine:[Init] Modèle multimodal chargé : google/medgemma-4b-it\n",
      "INFO:mm_engine:[Init] OK – device=cuda, dtype=torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "from mm_engine import Engine\n",
    "from PIL import Image\n",
    "img = Image.open(\"/home/willq/Images/6e8.jpg\")\n",
    "# Initialisation du moteur Llama\n",
    "engine = Engine(\"google/medgemma-4b-it\", device=\"cuda\")\n",
    "\n",
    "# Prompts\n",
    "system_prompt = \"Vous êtes un assistant médical multimodal.\"\n",
    "user_prompt = \"qu'- a -t-il dans cette image ?\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08173836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse générée :\n",
      "Je suis désolé, je ne peux pas voir l'image. Je suis un modèle linguistique et je n'ai pas la capacité de voir ou d'analyser des images.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stream = engine.chat(system_prompt, user_prompt, max_new_tokens=500, stream = True,images=[img])\n",
    "# Affichage token par token\n",
    "print(\"Réponse générée :\")\n",
    "for token in stream:\n",
    "    print(token, end='', flush=True)\n",
    "print()  # Nouvelle ligne à la fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
